use derive_builder::Builder;
use serde::{Deserialize, Serialize};

/* openai request body example
{
  "model": "gpt-3.5-turbo",
  "temperature": 0.2,
  "messages": [
    {"role": "system", "content": "...prompt..."},
    {"role": "user", "content": "concurrence"}]
}
 */
#[derive(Default, Debug, Serialize, Deserialize, Builder)]
#[builder(setter(into), default)]
pub struct OpenAIRequest {
    /// ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported.
    pub model: String,

    pub messages: Vec<Message>,

    /// What sampling temperature to use, between 0 and 2.
    /// Higher values like 0.8 will make the output more random,
    /// while lower values like 0.2 will make it more focused and deterministic.
    /// We generally recommend altering this or top_p but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>, // default 0.2

    /// An alternative to sampling with temperature,
    /// called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// How many chat completion choices to generate for each input message.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<u32>, // default 1

    /// If set, partial message deltas will be sent, like in ChatGPT.
    /// Tokens will be sent as data-only server-sent events as they become available,
    /// with the stream terminated by a data: [DONE] message.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop: Option<Vec<String>>,

    /// The maximum number of tokens allowed for the generated answer.
    /// By default, the number of tokens the model can return will be (4096 - prompt tokens).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,

    /// Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on whether they appear in the text so far,
    /// increasing the model's likelihood to talk about new topics.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,

    /// Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to
    /// an associated bias value from -100 to 100. Mathematically,
    /// the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model,
    /// but values between -1 and 1 should decrease or increase likelihood of selection;
    /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logit_bias: Option<String>,

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

#[derive(Default, Debug, Clone, Serialize, Deserialize, Builder)]
#[builder(setter(into))]
pub struct Message {
    /// role of the message, either "user" or "system"
    role: String,

    /// content of the message
    content: String,
}

/*
{
  "id": "chatcmpl-6sjIQhzXydbCQknCQfdfVK7PwEkQT",
  "object": "chat.completion",
  "created": 1678501850,
  "model": "gpt-3.5-turbo-0301",
  "usage": {
    "prompt_tokens": 75,
    "completion_tokens": 185,
    "total_tokens": 260
  },
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "..."
      },
      "finish_reason": "stop",
      "index": 0
    }
  ]
}
*/

#[derive(Debug, Deserialize)]
pub struct OpenAIResponse {
    pub id: String,
    pub object: String,
    pub created: u64,
    pub model: String,
    pub usage: Usage,
    pub choices: Vec<Choice>,
}

#[derive(Debug, Deserialize)]
pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

#[derive(Debug, Deserialize)]
pub struct Choice {
    pub message: Message,
    pub finish_reason: Option<String>,
    pub index: u32,
}

impl OpenAIResponse {
    pub fn parse_content<T, F>(&self, f_parse: F) -> anyhow::Result<T>
    where
        F: Fn(String) -> anyhow::Result<T>,
    {
        let choice = self.choices.get(0).ok_or(anyhow::anyhow!("no choice"))?;
        let content = choice.message.content.as_str();
        let content = content.trim();
        f_parse(content.to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    const RESPONSE: &str = r#"{
			"id": "chatcmpl-6sjIQhzXydbCQknCQfdfVK7PwEkQT",
			"object": "chat.completion",
			"created": 1678501850,
			"model": "gpt-3.5-turbo-0301",
			"usage": {
			  "prompt_tokens": 75,
			  "completion_tokens": 185,
			  "total_tokens": 260
			},
			"choices": [
			  {
				"message": {
				  "role": "assistant",
				  "content": "abc\n\n123\n\n99"
				},
				"finish_reason": "stop",
				"index": 0
			  }
			]
		  }"#;

    #[test]
    fn builder_should_work() {
        let req = OpenAIRequestBuilder::default()
            .model("gpt-3.5-turbo".to_string())
            .messages(vec![MessageBuilder::default()
                .role("system".to_string())
                .content("prompt".to_string())
                .build()
                .unwrap()])
            .temperature(0.2)
            .build()
            .unwrap();

        let s = serde_json::to_string(&req).unwrap();
        assert_eq!(
            s,
            r#"{"model":"gpt-3.5-turbo","messages":[{"role":"system","content":"prompt"}],"temperature":0.2}"#
        );
    }

    #[test]
    fn parse_content_should_work() {
        #[derive(Debug, Deserialize, PartialEq, Eq)]
        struct ParseResult {
            // abc
            s: String,
            // 123\n\n99
            n: String,
        }
        let response: OpenAIResponse = serde_json::from_str(RESPONSE).unwrap();

        let x = response
            .parse_content::<ParseResult, _>(|s| {
                let re = regex::Regex::new(r"^(?P<s>.*)(\n)+(?P<n>(.|\s)+)$").unwrap();
                let caps = re.captures(&s).ok_or(anyhow::anyhow!("no match"))?;
                Ok(ParseResult {
                    s: caps.name("s").unwrap().as_str().into(),
                    n: caps.name("n").unwrap().as_str().into(),
                })
            })
            .unwrap();
        assert_eq!(
            x,
            ParseResult {
                s: "abc".to_string(),
                n: "123\n\n99".to_string()
            }
        )
    }
}
